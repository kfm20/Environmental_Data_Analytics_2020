---
title: "12: Generalized Linear Models (Linear Regression)"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2020"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## Objectives
2. Apply special cases of the GLM (linear regression) to real datasets
3. Interpret and report the results of linear regressions in publication-style formats
3. Apply model selection methods to choose model formulations

## Set up
```{r, message = FALSE}
getwd()
library(tidyverse)
options(scipen = 4) #allows numbers to display 4 digits so we don;t have to interpret notation of e^4 etc. Effects coeff from statistical tests and axes of figures

PeterPaul.chem.nutrients <- read.csv("./Data/Processed/NTL-LTER_Lake_Chemistry_Nutrients_PeterPaul_Processed.csv")

# Set theme
mytheme <- theme_classic(base_size = 14) +
  theme(axis.text = element_text(color = "black"), 
        legend.position = "top")
theme_set(mytheme)
```

## Linear Regression
The linear regression, like the t-test and ANOVA, is a special case of the **generalized linear model** (GLM). A linear regression is comprised of a continuous response variable, plus a combination of 1+ continuous response variables (plus the error term). The deterministic portion of the equation describes the response variable as lying on a straight line, with an intercept and a slope term. The equation is thus a typical algebraic expression: 
$$ y = \alpha + \beta*x + \epsilon $$
>For each coeff, we will have a different beta and alpha

The goal for the linear regression is to find a **line of best fit**, which is the line drawn through the bivariate space that minimizes the total distance of points from the line. This is also called a >>> "least squares" regression.<<< The remainder of the variance not explained by the model is called the **residual error.** 

The linear regression will test the null hypotheses that
1. The intercept (alpha) is equal to zero.
2. The slope (beta) is equal to zero

Whether or not we care about the result of each of these tested hypotheses will depend on our research question. Sometimes, the test for the intercept will be of interest, and sometimes it will not.

Important components of the linear regression are the correlation and the R-squared value. The **correlation** is a number between -1 and 1, describing the relationship between the variables. 
>>>Correlations close to -1 represent strong negative correlations, correlations close to zero represent weak correlations, and correlations close to 1 represent strong positive correlations. <<< Helps determine direction of relationship
The **R-squared value** is the correlation squared, becoming a number between 0 and 1. The R-squared value describes the percent of variance accounted for by the explanatory variables. Describes strength of relationship

## Simple Linear Regression
For the NTL-LTER dataset, can we predict irradiance (light level) from depth?
Guessing the deeper we are the darker it will be
```{r}
irradiance.regression <- lm(PeterPaul.chem.nutrients$irradianceWater ~ PeterPaul.chem.nutrients$depth)
# another way to format the lm function, doing the same thing but different formatting
irradiance.regression <- lm(data = PeterPaul.chem.nutrients, irradianceWater ~ depth)
summary(irradiance.regression)
#>REsults- depth is a continuous variable so only gives one coefficient
#WE are interested in knowing if it gets darker as you go lower in water, interested in depth pvalue
#adjusted r squared says about 30% of variance is explained by depth

# Correlation
cor.test(PeterPaul.chem.nutrients$irradianceWater, PeterPaul.chem.nutrients$depth)
#Results- correlation of -.55, if you square this you get adjusted r square found from test above

```
Question: How would you report the results of this test (overall findings and report of statistical output)?

>  At greated depth, irrdadiance decreases (linear regression, R2=0.31, df=15,449, p<0.0001)
>Depth accounts for approximately 30% of variance in lake irradiance. (linear regression, R2=0.31, df=15,449, p<0.0001)
> Irradiance decreases significantly with increasing depth (linear regression, R2=0.31, df=15,449, p<0.0001)
>For each 1m increase in depth, irradiance decreases by 95 units (linear regression, R2=0.31, df=15,449, p<0.0001)

So, we see there is a significant negative correlation between irradiance and depth (lower light levels at greater depths), and that this model explains about 31 % of the total variance in irradiance. Let's visualize this relationship and the model itself. 

An exploratory option to visualize the model fit is to use the function `plot`. This function will return four graphs, which are intended only for checking the fit of the model and not for communicating results. The plots that are returned are: 

1. **Residuals vs. Fitted.** The value predicted by the line of best fit is the fitted value, and the residual is the distance of that actual value from the predicted value. By definition, there will be a balance of positive and negative residuals. Watch for drastic asymmetry from side to side or a marked departure from zero for the red line - these are signs of a poor model fit.

2. **Normal Q-Q.** The points should fall close to the 1:1 line. We often see departures from 1:1 at the high and low ends of the dataset, which could be outliers. 

3. **Scale-Location.** Similar to the residuals vs. fitted graph, this will graph the squared standardized residuals by the fitted values. 

4. **Residuals vs. Leverage.** This graph will display potential outliers. The values that fall outside the dashed red lines (Cook's distance) are outliers for the model. Watch for drastic departures of the solid red line from horizontal - this is a sign of a poor model fit.

```{r, fig.height = 3, fig.width = 4}
par(mfrow = c(2,2), mar=c(1,1,1,1)) 
plot(irradiance.regression)
par(mfrow = c(1,1)) #allows to go back to normal structure, otherwise would keep gridding them

#only use is for verifying assumptions of our test, should never end up in a paper

#want residuals vs fitted to have an even horiz red line with not much data variance, scale location want horiz and not much clustering, cooks distance to tell of outliers
```

The option best suited for communicating findings is to plot the explanatory and response variables as a scatterplot. 

```{r, fig.height = 3, fig.width = 4}
# Plot the regression
irradiancebydepth <- 
  ggplot(PeterPaul.chem.nutrients, aes(x = depth, y = irradianceWater)) +
  ylim(0, 2000) +  #there is one MAJOR outlier, so we need to limit y axis
  geom_point() 
print(irradiancebydepth) 

#Results- we can see an exponential decay relationship as we go down in depth
```

Given the distribution of irradiance values, we don't have a linear relationship between x and y in this case. Let's try log-transforming the irradiance values.
#Log Transform
Totally allowable to try and make a best fit

Values of zero will give error, so here we remove the 3 points, which won't affect our statistical robustness, otherwise could put them as something soo small so they don;t alter and we don't have to remove

These are interpretted differently than regular linear

```{r, fig.height = 3, fig.width = 4}
PeterPaul.chem.nutrients <- filter(PeterPaul.chem.nutrients, 
                                   irradianceWater != 0 & irradianceWater < 5000)
#filtering for irradiance values that are not zero and those that are less than 5,000
#If we log, we might better approximate linear relationship...
irradiance.regression2 <- lm(data = PeterPaul.chem.nutrients, log(irradianceWater) ~ depth)
summary(irradiance.regression2)
#Results- now depth responsible for 71% of change in irradiance as depth increases

par(mfrow = c(2,2), mar=c(1,1,1,1))
plot(irradiance.regression2)
par(mfrow = c(1,1))
#Again, checking assumptions and normalities

# Add a line and standard error for the linear regression
irradiancebydepth2 <- 
  ggplot(PeterPaul.chem.nutrients, aes(x = depth, y = irradianceWater)) +
  geom_smooth(method = "lm") + #plots line of best fit
  scale_y_log10() + #changes y axis scale instead of logging the variable itself
  geom_point() 
print(irradiancebydepth2) 

# Standard Error can also be removed
irradiancebydepth2 <- 
    ggplot(PeterPaul.chem.nutrients, aes(x = depth, y = irradianceWater)) +
    geom_point() +
    scale_y_log10() +
    geom_smooth(method = 'lm', se = FALSE, color = "black") #removes standard error and puts it in black. It is the standard error of the model, not the data
print(irradiancebydepth2)

# Make the graph attractive
irradiancebydepth2 <- 
    ggplot(PeterPaul.chem.nutrients, aes(x = depth, y = irradianceWater)) +
    geom_point(pch= 1, alpha=0.5, color= "dodgerblue") + #changing point type transparency,and color
    scale_y_log10() +
  labs(x="Depth (m)", y="Irradiance ()")+      #Adding new x and y axis labels
    geom_smooth(method = 'lm', se = FALSE, color = "black") 
print(irradiancebydepth2)
```

## Non-parametric equivalent: Spearman's Rho
As with the t-test and ANOVA, there is a nonparametric variant to the linear regression. The **Spearman's rho** test has the advantage of >>>not depending on the normal distribution,<<< but this test is not as robust as the linear regression.

``` {r}
cor.test(PeterPaul.chem.nutrients$irradianceWater, PeterPaul.chem.nutrients$depth, 
         method = "spearman", exact = FALSE)
#exact =True/False tells if we want exact or estimate of pvalue
```

## Multiple Regression
It is possible, and often useful, to consider multiple continuous explanatory variables at a time in a linear regression. For example, total phosphorus concentration in Paul Lake (the unfertilized lake- reference system) could be dependent on depth and dissolved oxygen concentration: 

``` {r, fig.height = 3, fig.width = 4}
TPregression <- lm(data = subset(PeterPaul.chem.nutrients, lakename == "Paul Lake"), 
                   tp_ug ~ depth + dissolvedOxygen)
#predicting TP, might depend on depth, DO-- generating overall alpha value, intercept, representing depth=0(surface), and DO=0, with two beta's, one depth and one DO

summary(TPregression)
#Result- depth IS a significant predictor of TP, DO is also a significant predictor of TP
#adjusted R squared, depth and DO explaining 29% of variance in TP

TPplot <- ggplot(subset(PeterPaul.chem.nutrients, lakename == "Paul Lake"), 
                 aes(x = dissolvedOxygen, y = tp_ug, color = depth)) +
  geom_point() +
  xlim(0, 20)
print(TPplot)
#WE see.. visually not showing any sort of relationship

#Let's switch which one is the color, this shows us more of a relationship between TP and depth, but the color aspect of DO falls apart
TPplot <- ggplot(subset(PeterPaul.chem.nutrients, lakename == "Paul Lake"), 
                 aes(x = depth, y = tp_ug, color = dissolvedOxygen)) +
  geom_point() +
  xlim(0, 20)
print(TPplot)


par(mfrow = c(2,2), mar=c(1,1,1,1))
plot(TPregression)
par(mfrow = c(1,1))

```

## Correlation Plots
We can also make exploratory plots of several continuous data points to determine possible relationships, as well as covariance among explanatory variables. 
#Do any of the explanatory variables have a relationship with eachother
Don;t work well with NAs
```{r, fig.height = 3, fig.width = 4}
install.packages("corrplot")
library(corrplot)

PeterPaulnutrients <- 
  PeterPaul.chem.nutrients %>%
  select(tn_ug:po4) %>% #grabbing only columns in data set with nutrients
  na.omit() #omitting any rows with an NA, displaying only complete rows

PeterPaulCorr <- cor(PeterPaulnutrients) #run this corr function to turn it into a correlation matrix

corrplot(PeterPaulCorr, method = "ellipse") #using corrplot on correlation matrix ^
#ellipse of directionality, weaker correlations are more circular, stronger are skinnier

corrplot.mixed(PeterPaulCorr, upper = "ellipse")
#a way to customize the output
```

## AIC to select variables
#Continue exploring idea of predicting TP in Paul Lake
However, it is possible to over-parameterize a linear model. Adding additional explanatory variables takes away degrees of freedom, and if explanatory variables co-vary the interpretation can become overly complicated. Remember, an ideal statistical model balances simplicity and explanatory power! To help with this tradeoff, we can use the **Akaike's Information Criterion (AIC)** to compute a stepwise regression that either adds explanatory variables from the bottom up or removes explanatory variables from a full set of suggested options. >>>The smaller the AIC value, the better.<<<
The AIC numbers can't be compared across models, only useful in one we are working in, don't mean anything otherwise

Let's say we want to know which explanatory variables will allow us to best predict total phosphorus concentrations. Potential explanatory variables from the dataset could include depth, dissolved oxygen, temperature, PAR, total N concentration, and phosphate concentration.

```{r}
Paul.naomit <- PeterPaul.chem.nutrients %>%
  filter(lakename == "Paul Lake") %>%
  na.omit() #omitting any rows that even contain one NA


TPAIC <- lm(data = Paul.naomit, tp_ug ~ depth + dissolvedOxygen + 
              temperature_C + tn_ug + po4) 
#choosing any potential variables that might serve as a predictor


step(TPAIC) 
#runs a series of AIC tests, takes potential predictor variables out of model and then creates AIC, model with lower AIC value excluded, does it again without that one and runs again, removes next lowest, runs again, ... repeats until determines removing none will be better-- provides final coefficients


#Need to create new formula based on the outputs...below

TPmodel <- lm(data = Paul.naomit, tp_ug ~ dissolvedOxygen + temperature_C + tn_ug)
summary(TPmodel)
#Results: estimating 26% of variance, last lowest variable might not be significant predictor, but it contributes to a significant amount of variance
```

#Interpretation
> DO, and Temperature are significant predictors of TP
> DO and Temp account for 25% of the variance of TP